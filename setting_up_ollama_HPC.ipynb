{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cad754-096f-4e8e-906d-e23e7318128a",
   "metadata": {},
   "source": [
    "# Setting up Ollama with your HPC Account\n",
    "\n",
    "Ollama is an open-source tool that enables users to easily download, install, and run large language models locally on their own hardware, eliminating the need for cloud-based API calls or external services. When setting up Ollama for an HPC (High-Performance Computing) account, you're essentially configuring this lightweight runtime environment to leverage the substantial computational resources available in HPC clusters, such as powerful GPUs and high-memory nodes. The setup process typically involves downloading the Ollama binary to your HPC user directory, ensuring proper GPU drivers and CUDA compatibility, and then pulling your desired models (like Llama 2, Mistral, or CodeLlama) which will be stored locally on the cluster's storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3562-2b47-45ca-947c-a1970b54747b",
   "metadata": {},
   "source": [
    "### 1. Download Ollama (we need the Linux Version for the HPC System)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce30329-31ce-47cb-bc52-add8e0f46e23",
   "metadata": {},
   "source": [
    "Use your AcademicID username and HPC User ID. You can find the information regarding your person here:https://hpcproject.gwdg.de/projects/baaa32d3-6b49-4831-8b75-87ff44056ae0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6db1a-36d9-43c0-86e8-4a865c20a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Linux version for the HPC system\n",
    "!wget -O /user/sarah.oberbichler/u18915/ollama-linux-amd64.tgz https://github.com/ollama/ollama/releases/download/v0.11.4/ollama-linux-amd64.tgz\n",
    "\n",
    "# Extract it\n",
    "!cd /user/sarah.oberbichler/u18915 && tar -xzf ollama-linux-amd64.tgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0811187-18aa-4c72-98d0-9f3dc37cf1ff",
   "metadata": {},
   "source": [
    "### Set up your Environment\n",
    "\n",
    "First, see what ports are available for your environment. Copy the number and paste it after ['OLLAMA_HOST'] = '127.0.0.1: in the cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d4e39-9dae-40da-b541-b12c80f5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def find_free_ports(start=11434, end=11450):\n",
    "    free_ports = []\n",
    "    for port in range(start, end + 1):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('127.0.0.1', port)) != 0:\n",
    "                free_ports.append(port)\n",
    "    return free_ports\n",
    "\n",
    "available_ports = find_free_ports()\n",
    "print(\"✅ Available ports:\", available_ports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbf6f3-35f8-409e-aa02-69181ab9c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up your environment\n",
    "os.environ['OLLAMA_HOME'] = '/user/sarah.oberbichler/u18915/.ollama' #/workspace/ceph-hdd/.ollama'\n",
    "os.environ['PATH'] = f\"/user/sarah.oberbichler/u18915/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/opt/conda/lib/python3.12/site-packages/nvidia/cuda_runtime/lib:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8afe8-c2a0-40f3-b1fc-0d73a0340a94",
   "metadata": {},
   "source": [
    "### Start the Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bee8259-6140-4558-8ac9-90ddea9d59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started (PID: 4017882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-08T16:24:36.021+02:00 level=INFO source=routes.go:1304 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY:*.hlrn.de,jupyter.hpc.gwdg.de,jupyter.usr.hpc.gwdg.de,localhost,127.0.0.1 OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/user/sarah.oberbichler/u18915/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy:http://www-cache.gwdg.de:3128 https_proxy:http://www-cache.gwdg.de:3128 no_proxy:]\"\n",
      "time=2025-09-08T16:24:36.050+02:00 level=INFO source=images.go:477 msg=\"total blobs: 15\"\n",
      "time=2025-09-08T16:24:36.145+02:00 level=INFO source=images.go:484 msg=\"total unused blobs removed: 10\"\n",
      "time=2025-09-08T16:24:36.152+02:00 level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
      "time=2025-09-08T16:24:36.161+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-09-08T16:24:36.982+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-62796ba6-1f72-81a0-92ab-4754c170ca42 library=cuda variant=v12 compute=8.0 driver=12.4 name=\"NVIDIA A100-SXM4-40GB\" total=\"39.4 GiB\" available=\"39.0 GiB\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/08 - 16:24:44 | 200 |      50.929µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/08 - 16:24:44 | 200 |    4.147026ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "Server status: 0\n",
      "Models: NAME                                              ID              SIZE     MODIFIED    \n",
      "MHKetbi/allenai_OLMo2-0325-32B-Instruct:q5_K_S    7c99a6b3ca4d    22 GB    2 hours ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Kill any existing processes\n",
    "!pkill -f ollama\n",
    "time.sleep(2)\n",
    "\n",
    "# Start server\n",
    "process = subprocess.Popen([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'serve'\n",
    "], env=os.environ.copy())\n",
    "\n",
    "print(f\"Server started (PID: {process.pid})\")\n",
    "time.sleep(8)\n",
    "\n",
    "# Test server\n",
    "result = subprocess.run([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'list'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(f\"Server status: {result.returncode}\")\n",
    "print(f\"Models: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7ca79-51b7-4f6d-96a9-0763fca6784d",
   "metadata": {},
   "source": [
    "### Check the GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887e2551-7d34-4139-b25c-fbbb23fc3976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB]\n",
      "NVIDIA A100-SXM4-40GB, 40960 MiB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a3489-47ad-45d8-8176-24eaf7b83a31",
   "metadata": {},
   "source": [
    "### Download a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cb7ea-5a0e-429e-8c9a-9d2532824439",
   "metadata": {},
   "source": [
    "We will use the open source model OLMo2-0325-32B-Instruct. OLMo2 is a 32-billion parameter transformer-based language model developed by the Allen Institute for AI (AI2), released in March 2025 as part of the OLMo 2 family. This instruction-tuned variant represents a significant milestone as the first fully-open model to outperform GPT-3.5 Turbo and GPT-4o mini on a suite of popular academic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b62e82-20f5-4510-a8fb-df019211b632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n",
      "[GIN] 2025/09/08 - 16:24:54 | 200 |       28.32µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/08 - 16:24:55 | 200 |  930.180634ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "Download result: 0\n",
      "Download output: \u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling fd251f9b7719: 100% ▕██████████████████▏  22 GB                         \u001b[K\n",
      "pulling f780e20cf629: 100% ▕██████████████████▏  394 B                         \u001b[K\n",
      "pulling cf989ea142f2: 100% ▕██████████████████▏   10 B                         \u001b[K\n",
      "pulling fe8aa96f823c: 100% ▕██████████████████▏   52 B                         \u001b[K\n",
      "pulling 7363dd993a67: 100% ▕██████████████████▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if result.returncode == 0:  # If server is working\n",
    "    print(\"Downloading model...\")\n",
    "    download_result = subprocess.run([\n",
    "        '/user/n.rastinger/u20065/bin/ollama', 'pull', 'MHKetbi/allenai_OLMo2-0325-32B-Instruct:q5_K_S'\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    print(f\"Download result: {download_result.returncode}\")\n",
    "    if download_result.stderr:\n",
    "        print(f\"Download output: {download_result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed2383-96b8-48a8-9548-81885e43317a",
   "metadata": {},
   "source": [
    "### Get the model started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee76c095-2956-4e6c-bf8c-5a214c01e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL for your local Ollama server\n",
    "API_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "#Set model and model parameters\n",
    "def ask_olmo_api(prompt, model=\"MHKetbi/allenai_OLMo2-0325-32B-Instruct:q5_K_S\"):\n",
    "    import urllib.request, json\n",
    "    req = urllib.request.Request(\n",
    "        f\"{API_URL}/api/chat\",\n",
    "        json.dumps({\"model\": model, \"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
    "                    \"stream\": False, \"temperature\": 0.2}).encode(),\n",
    "        {\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    with urllib.request.urlopen(req) as r:\n",
    "        return json.loads(r.read())[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb776c97-a0ea-4564-99a9-f1f9a238a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-08T16:25:18.769+02:00 level=INFO source=sched.go:786 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 gpu=GPU-62796ba6-1f72-81a0-92ab-4754c170ca42 parallel=1 available=41840410624 required=\"23.0 GiB\"\n",
      "time=2025-09-08T16:25:19.088+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"503.8 GiB\" free=\"477.4 GiB\" free_swap=\"0 B\"\n",
      "time=2025-09-08T16:25:19.090+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=\"\" memory.available=\"[39.0 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"23.0 GiB\" memory.required.partial=\"23.0 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[23.0 GiB]\" memory.weights.total=\"20.4 GiB\" memory.weights.repeating=\"20.0 GiB\" memory.weights.nonrepeating=\"402.0 MiB\" memory.graph.full=\"853.3 MiB\" memory.graph.partial=\"853.3 MiB\"\n",
      "llama_model_loader: loaded meta data with 36 key-value pairs and 707 tensors from /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Open_Instruct_Dev\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Allenai\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   7:                  general.base_model.0.name str              = OLMo 2 0325 32B DPO\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Allenai\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/allenai/OLMo-2...\n",
      "llama_model_loader: - kv  10:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv  11:                     general.dataset.0.name str              = Tulu 3 Sft Olmo 2 Mixture\n",
      "llama_model_loader: - kv  12:             general.dataset.0.organization str              = Allenai\n",
      "llama_model_loader: - kv  13:                 general.dataset.0.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv  15:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  16:                          olmo2.block_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       olmo2.context_length u32              = 4096\n",
      "llama_model_loader: - kv  18:                     olmo2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  19:                  olmo2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  20:                 olmo2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  21:              olmo2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  22:                       olmo2.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  23:     olmo2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 100277\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 16\n",
      "llama_model_loader: - type  f32:  257 tensors\n",
      "llama_model_loader: - type q5_K:  449 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Small\n",
      "print_info: file size   = 20.71 GiB (5.52 BPW) \n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.6143 MB\n",
      "print_info: arch             = olmo2\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 32.23 B\n",
      "print_info: general.name     = Open_Instruct_Dev\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 100352\n",
      "print_info: n_merges         = 100000\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOT token        = 100257 '<|endoftext|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100277 '<|pad|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100265 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-09-08T16:25:19.257+02:00 level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/mnt/vast-nhr/home/sarah.oberbichler/u18915/bin/ollama runner --model /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 --ctx-size 4096 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 1 --port 35435\"\n",
      "time=2025-09-08T16:25:19.258+02:00 level=INFO source=sched.go:481 msg=\"loaded runners\" count=1\n",
      "time=2025-09-08T16:25:19.258+02:00 level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-09-08T16:25:19.258+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
      "time=2025-09-08T16:25:19.283+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
      "load_backend: loaded CUDA backend from /mnt/vast-nhr/home/sarah.oberbichler/u18915/lib/ollama/libggml-cuda.so\n",
      "load_backend: loaded CPU backend from /mnt/vast-nhr/home/sarah.oberbichler/u18915/lib/ollama/libggml-cpu-haswell.so\n",
      "time=2025-09-08T16:25:20.548+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-09-08T16:25:20.552+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:35435\"\n",
      "time=2025-09-08T16:25:20.763+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-40GB) - 39902 MiB free\n",
      "llama_model_loader: loaded meta data with 36 key-value pairs and 707 tensors from /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Open_Instruct_Dev\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Allenai\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   7:                  general.base_model.0.name str              = OLMo 2 0325 32B DPO\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Allenai\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/allenai/OLMo-2...\n",
      "llama_model_loader: - kv  10:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv  11:                     general.dataset.0.name str              = Tulu 3 Sft Olmo 2 Mixture\n",
      "llama_model_loader: - kv  12:             general.dataset.0.organization str              = Allenai\n",
      "llama_model_loader: - kv  13:                 general.dataset.0.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv  15:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  16:                          olmo2.block_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       olmo2.context_length u32              = 4096\n",
      "llama_model_loader: - kv  18:                     olmo2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  19:                  olmo2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  20:                 olmo2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  21:              olmo2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  22:                       olmo2.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  23:     olmo2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 100277\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 16\n",
      "llama_model_loader: - type  f32:  257 tensors\n",
      "llama_model_loader: - type q5_K:  449 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Small\n",
      "print_info: file size   = 20.71 GiB (5.52 BPW) \n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.6143 MB\n",
      "print_info: arch             = olmo2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 64\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 27648\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 32B\n",
      "print_info: model params     = 32.23 B\n",
      "print_info: general.name     = Open_Instruct_Dev\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 100352\n",
      "print_info: n_merges         = 100000\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOT token        = 100257 '<|endoftext|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100277 '<|pad|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100265 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "time=2025-09-08T16:25:51.460+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
      "time=2025-09-08T16:25:51.711+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "load_tensors: offloading 64 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 65/65 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size = 20865.97 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   336.88 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context:  CUDA_Host  output buffer size =     0.40 MiB\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context:      CUDA0 compute buffer size =   358.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    18.01 MiB\n",
      "llama_context: graph nodes  = 2438\n",
      "llama_context: graph splits = 2\n",
      "time=2025-09-08T16:26:39.390+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 80.11 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/08 - 16:27:01 | 200 |         1m42s |       127.0.0.1 | POST     \"/api/chat\"\n",
      ".[GIN] 2025/09/08 - 16:27:01 | 200 |  262.257514ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      ".[GIN] 2025/09/08 - 16:27:01 | 200 |  156.107969ms |       127.0.0.1 | POST     \"/api/chat\"\n",
      ".\n",
      "Warmup complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the model \n",
    "def warm_up_model():\n",
    "    \"\"\"Warm up the model with simple prompts.\"\"\"\n",
    "    print(\"Warming up model...\")\n",
    "    \n",
    "    for prompt in [\"Hi\", \"What is 2+2?\", \"Say hello\"]:\n",
    "        try:\n",
    "            ask_olmo_api(prompt)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\nWarmup complete!\")\n",
    "\n",
    "warm_up_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc00cd-f7ea-47f1-b260-79f33289f85e",
   "metadata": {},
   "source": [
    "### Chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0aba6c-1cca-4979-8ed1-236693229354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/08 - 16:27:01 | 200 |  120.111007ms |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_olmo_api('Say Hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
