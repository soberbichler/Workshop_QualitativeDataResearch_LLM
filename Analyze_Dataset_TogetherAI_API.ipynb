{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2f0pNCnVB25qBfIUArESh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/Analyze_Dataset_TogetherAI_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Requirements to Use Models via Togehter.AI\n",
        "\n",
        "**Together.ai** is an API service that hosts and provides access to various open-source large language models through a unified interface that mimics OpenAI's API format. Instead of running models yourself or using only proprietary options like GPT-4, you can make API calls to models like Llama, Mixtral, or other open-source alternatives that Together.ai runs on their infrastructure. The service essentially acts as a middleman - they handle the compute and model hosting while you pay per token for inference, similar to how OpenAI works but with different models and typically lower costs. It's useful when you want to use open-source models without setting up your own GPU infrastructure, though you're still dependent on their service availability and pricing structure."
      ],
      "metadata": {
        "id": "_h-F0eMD3ugY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "\n",
        "We will use the a version of the Llama 3.3 70B model."
      ],
      "metadata": {
        "id": "1DdKG2PhCP3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ.get(\"TOGETHER_API_KEY\")\n"
      ],
      "metadata": {
        "id": "qL4Xd8Q8LvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "from together import Together"
      ],
      "metadata": {
        "id": "r7_zGPsTCuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset\n",
        "\n",
        "You can find it here: https://github.com/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/data/SummerSchool_dataset.xlsx"
      ],
      "metadata": {
        "id": "lWanAxwivcEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('SummerSchool_dataset.xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k1kM3w5kvROr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the Dataset\n",
        "\n",
        "\n",
        "> ***After running the cell, fill in the model documentation while waiting (and continue after saving the results)!***\n",
        "\n",
        "\n",
        "\n",
        "Model documentation: https://seafile.rlp.net/seafhttp/f/a5b34ec61267408da431/?op=view"
      ],
      "metadata": {
        "id": "yVI26h2SHhpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC6ir-H52_5d"
      },
      "outputs": [],
      "source": [
        "from together import Together\n",
        "import pandas as pd\n",
        "\n",
        "def ask_llama_api(text, temperature=0.1, max_tokens=1000, random_seed =1):\n",
        "    \"\"\"Simple Llama API call using Together API\"\"\"\n",
        "    prompt = f\"\"\"You are an expert at analyzing historical texts and you dislike to summarize. Please find argumentative units in historical articles. Arguments in newspapers are often implicit but should contain a clear premise (with an inclusice claim)\n",
        "OUTPUT FORMAT - EXACTLY these 4 XML tags and NOTHING else:\n",
        "<argument>Original argument text OR \"NA\"</argument>\n",
        "<claim>Core claim (implication) in one sentence OR \"NA\"</claim>\n",
        "<explanation>Why this is an argument OR \"NA\"</explanation>\n",
        "<human_verification_needed>True OR False</human_verification_needed>\n",
        "EXAMPLE WITH ARGUMENT:\n",
        "<argument>Es sind furchtbare Bilder, die sich dabei entrollen. Unter den Trümmern des einen Hause», so erzählt Luigt Barsint im Corrtcre della sera, findet man die Leichen von Unglück lichen, die in anderen Häusern gewohnt baben und die in der Ber- Wirrung de» schrcck.ichen Augenblickes instinktiv bet Fremden Hülfe und Unterschlupf suchten. Niemand erkennt jetzt diese armen Ein dringlinge, ihre Leichen werden nicht reklamiert, und man trägt sie hinunter an de» Strand, wo sie in langer Reihe einer neben den anderen hingebettet weiden, in denselben Tüchern und Decken, in denen sie tbren Tod gesunden.</argument>\n",
        "<claim>The earthquake's chaos led to unidentified victims dying in unfamiliar places.</claim>\n",
        "<explanation>Describes how people fled to other houses seeking help during the disaster, died there, and now cannot be identified or claimed by relatives. Shows cause (panic/confusion) and effect (anonymous deaths).</explanation>\n",
        "<human_verification_needed>False</human_verification_needed>\n",
        "EXAMPLE WITHOUT ARGUMENT:\n",
        "<argument>NA</argument>\n",
        "<claim>NA</claim>\n",
        "<explanation>NA</explanation>\n",
        "<human_verification_needed>FALSE</human_verification_needed>\n",
        "RULES:\n",
        "- NO SUMMARY; ONLY ORIGINAL EXTRACTOM FROM THE TEXT; don't extract anything that is not in the text. Only extract word by word\n",
        "- ONLY output these 4 XML tags\n",
        "- Factual reportings such as \"Dem Vulkanausbruch folgten drei Sturzwellen in etwa 10 Meter Höhe\" are NO arguments\n",
        "- Extract only original text without changes or use NA when you did not find an argument\n",
        "- The claim is not a translation of summary of argument. It should say what the (implicite) argument implies\n",
        "- In cases of uncertainty or ambiguity, say human_verification_needed TRUE\n",
        "- If no argument exists, use NA for all fields except <human_verification_needed>FALSE or TRUE</human_verification_needed>\n",
        "- More than one argumentative unit possible for one article, one unit has one clear claim and all the xml structures\n",
        "Extract the arguments from this text:\n",
        "{text}\"\"\"\n",
        "\n",
        "    client = Together()  # auth defaults to os.environ.get(\"TOGETHER_API_KEY\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            random_seed=random_seed\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "\n",
        "df['model_answer_llama'] = df['extracted_articles'].apply(lambda x: ask_llama_api(x))\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the Dataset and name it differently than \"results\""
      ],
      "metadata": {
        "id": "r69wSyhQI5ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('results.xlsx', index=False)"
      ],
      "metadata": {
        "id": "7CTGEfWJH9OJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}