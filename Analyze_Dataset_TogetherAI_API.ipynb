{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9utcTyCa4M8UsbiAjm4Uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/Analyze_Dataset_TogetherAI_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Requirements to Use Models via Togehter.AI\n",
        "\n",
        "**Together.ai** is an API service that hosts and provides access to various open-source large language models through a unified interface that mimics OpenAI's API format. Instead of running models yourself or using only proprietary options like GPT-4, you can make API calls to models like Llama, Mixtral, or other open-source alternatives that Together.ai runs on their infrastructure. The service essentially acts as a middleman - they handle the compute and model hosting while you pay per token for inference, similar to how OpenAI works but with different models and typically lower costs. It's useful when you want to use open-source models without setting up your own GPU infrastructure, though you're still dependent on their service availability and pricing structure."
      ],
      "metadata": {
        "id": "_h-F0eMD3ugY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "\n",
        "We will use a version of the Llama 3.3 70B model."
      ],
      "metadata": {
        "id": "1DdKG2PhCP3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ.get(\"TOGETHER_API_KEY\")\n"
      ],
      "metadata": {
        "id": "qL4Xd8Q8LvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "from together import Together"
      ],
      "metadata": {
        "id": "r7_zGPsTCuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset\n",
        "\n",
        "You can find it here: https://github.com/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/data/SummerSchool_dataset.xlsx"
      ],
      "metadata": {
        "id": "lWanAxwivcEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('SummerSchool_dataset.xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k1kM3w5kvROr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the Dataset\n",
        "\n",
        "\n",
        "> ***After running the cell, fill in the model documentation while waiting (and continue after saving the results)!***\n",
        "\n",
        "\n",
        "\n",
        "Model documentation: https://seafile.rlp.net/seafhttp/f/a5b34ec61267408da431/?op=view"
      ],
      "metadata": {
        "id": "yVI26h2SHhpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC6ir-H52_5d"
      },
      "outputs": [],
      "source": [
        "from together import Together\n",
        "import pandas as pd\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert at analyzing historical texts and you dislike to summarize.\n",
        "Your task: find argumentative units in historical articles.\n",
        "Arguments in newspapers are often implicit but should contain a clear premise (with an inclusive claim).\n",
        "\n",
        "OUTPUT FORMAT - EXACTLY these 4 XML tags and NOTHING else:\n",
        "<argument>Original argument text OR \"NA\"</argument>\n",
        "<claim>Core claim (implication) in one sentence OR \"NA\"</claim>\n",
        "<explanation>Why this is an argument OR \"NA\"</explanation>\n",
        "<human_verification_needed>True OR False</human_verification_needed>\n",
        "\n",
        "EXAMPLE WITH ARGUMENT:\n",
        "<argument>Es sind furchtbare Bilder, die sich dabei entrollen. Unter den Trümmern des einen Hauses, so erzählt Luigi Barsini im Corriere della Sera, findet man die Leichen von Unglücklichen, die in anderen Häusern gewohnt haben und die in der Verwirrung des schrecklichen Augenblickes instinktiv bei Fremden Hilfe und Unterschlupf suchten. Niemand erkennt jetzt diese armen Eindringlinge, ihre Leichen werden nicht reklamiert, und man trägt sie hinunter an den Strand, wo sie in langer Reihe einer neben den anderen hingebettet werden, in denselben Tüchern und Decken, in denen sie ihren Tod gefunden.</argument>\n",
        "<claim>The earthquake's chaos led to unidentified victims dying in unfamiliar places.</claim>\n",
        "<explanation>Describes how people fled to other houses seeking help during the disaster, died there, and now cannot be identified or claimed by relatives. Shows cause (panic/confusion) and effect (anonymous deaths).</explanation>\n",
        "<human_verification_needed>False</human_verification_needed>\n",
        "\n",
        "EXAMPLE WITHOUT ARGUMENT:\n",
        "<argument>NA</argument>\n",
        "<claim>NA</claim>\n",
        "<explanation>NA</explanation>\n",
        "<human_verification_needed>False</human_verification_needed>\n",
        "\n",
        "RULES:\n",
        "- NO SUMMARY; ONLY ORIGINAL EXCERPT FROM THE TEXT; don't extract anything that is not in the text. Only extract word by word.\n",
        "- ONLY output these 4 XML tags.\n",
        "- Factual reportings such as \"Dem Vulkanausbruch folgten drei Sturzwellen in etwa 10 Meter Höhe\" are NO arguments.\n",
        "- Extract only original text without changes or use NA when you did not find an argument.\n",
        "- The claim is not a translation or summary of the argument. It should say what the (implicit) argument implies.\n",
        "- In cases of uncertainty or ambiguity, set <human_verification_needed>True</human_verification_needed>.\n",
        "- If no argument exists, use NA for all fields except <human_verification_needed>.\n",
        "- More than one argumentative unit possible per article; one unit has one clear claim and all four XML fields.\n",
        "\"\"\"\n",
        "\n",
        "def ask_llama_api(text, temperature=0.1, max_tokens=1000, random_seed=1):\n",
        "    \"\"\"Simple Llama API call using Together API with system + user roles\"\"\"\n",
        "\n",
        "    client = Together()  # auth defaults to os.environ.get(\"TOGETHER_API_KEY\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"Extract arguments from this text:\\n{text}\"}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            random_seed=random_seed\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "\n",
        "df['model_answer_llama'] = df['extracted_articles'].apply(lambda x: ask_llama_api(x))\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the Dataset and name it differently than \"results\""
      ],
      "metadata": {
        "id": "r69wSyhQI5ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('results.xlsx', index=False)"
      ],
      "metadata": {
        "id": "7CTGEfWJH9OJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}