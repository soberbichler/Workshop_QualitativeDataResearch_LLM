{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtcOnrrB2Nvh90RP3JhoIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/TogetherAI_API_Setting_Up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Requirements to Use Models via Togehter.AI\n",
        "\n",
        "**Together.ai** is an API service that hosts and provides access to various open-source large language models through a unified interface that mimics OpenAI's API format. Instead of running models yourself or using only proprietary options like GPT-4, you can make API calls to models like Llama, Mixtral, or other open-source alternatives that Together.ai runs on their infrastructure. The service essentially acts as a middleman - they handle the compute and model hosting while you pay per token for inference, similar to how OpenAI works but with different models and typically lower costs. It's useful when you want to use open-source models without setting up your own GPU infrastructure, though you're still dependent on their service availability and pricing structure."
      ],
      "metadata": {
        "id": "_h-F0eMD3ugY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "\n",
        "We will use the a version of the Llama 3.3 70B model."
      ],
      "metadata": {
        "id": "1DdKG2PhCP3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ.get(\"TOGETHER_API_KEY\")\n"
      ],
      "metadata": {
        "id": "qL4Xd8Q8LvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together\n",
        "from together import Together"
      ],
      "metadata": {
        "id": "r7_zGPsTCuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">***Add model parameters to the model and set a random seed in order to make the results replicable.***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vA19NEVRC7Z9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC6ir-H52_5d"
      },
      "outputs": [],
      "source": [
        "client = Together() # auth defaults to os.environ.get(\"TOGETHER_API_KEY\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain machine learning\"\n",
        "      }\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ]
}