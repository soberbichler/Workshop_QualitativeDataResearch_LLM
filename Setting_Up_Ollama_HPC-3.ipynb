{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cad754-096f-4e8e-906d-e23e7318128a",
   "metadata": {},
   "source": [
    "# Setting up Ollama with your HPC Account\n",
    "\n",
    "Ollama is an open-source tool that enables users to easily download, install, and run large language models locally on their own hardware, eliminating the need for cloud-based API calls or external services. When setting up Ollama for an HPC (High-Performance Computing) account, you're essentially configuring this lightweight runtime environment to leverage the substantial computational resources available in HPC clusters, such as powerful GPUs and high-memory nodes. The setup process typically involves downloading the Ollama binary to your HPC user directory, ensuring proper GPU drivers and CUDA compatibility, and then pulling your desired models (like Llama 2, Mistral, or CodeLlama) which will be stored locally on the cluster's storage system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3562-2b47-45ca-947c-a1970b54747b",
   "metadata": {},
   "source": [
    "### 1. Download Ollama (we need the Linux Version for the HPC System)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce30329-31ce-47cb-bc52-add8e0f46e23",
   "metadata": {},
   "source": [
    "Use your AcademicID username and HPC User ID. You can find the information regarding your person here:https://hpcproject.gwdg.de/projects/baaa32d3-6b49-4831-8b75-87ff44056ae0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6db1a-36d9-43c0-86e8-4a865c20a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Linux version for the HPC system\n",
    "!wget -O /user/sarah.oberbichler/u18915/ollama-linux-amd64.tgz https://github.com/ollama/ollama/releases/download/v0.11.4/ollama-linux-amd64.tgz\n",
    "\n",
    "# Extract it\n",
    "!cd /user/sarah.oberbichler/u18915 && tar -xzf ollama-linux-amd64.tgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0811187-18aa-4c72-98d0-9f3dc37cf1ff",
   "metadata": {},
   "source": [
    "### Set up your Environment\n",
    "\n",
    "First, see what ports are available for your environment. Copy the number and paste it after ['OLLAMA_HOST'] = '127.0.0.1: in the cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d4e39-9dae-40da-b541-b12c80f5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def find_free_ports(start=11434, end=11450):\n",
    "    free_ports = []\n",
    "    for port in range(start, end + 1):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('127.0.0.1', port)) != 0:\n",
    "                free_ports.append(port)\n",
    "    return free_ports\n",
    "\n",
    "available_ports = find_free_ports()\n",
    "print(\"âœ… Available ports:\", available_ports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbf6f3-35f8-409e-aa02-69181ab9c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up your environment\n",
    "os.environ['OLLAMA_HOME'] = '/user/sarah.oberbichler/u18915/.ollama' #/workspace/ceph-hdd/.ollama'\n",
    "os.environ['PATH'] = f\"/user/sarah.oberbichler/u18915/bin:{os.environ.get('PATH', '')}\"\n",
    "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/opt/conda/lib/python3.12/site-packages/nvidia/cuda_runtime/lib:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8afe8-c2a0-40f3-b1fc-0d73a0340a94",
   "metadata": {},
   "source": [
    "### Start the Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee8259-6140-4558-8ac9-90ddea9d59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Kill any existing processes\n",
    "!pkill -f ollama\n",
    "time.sleep(2)\n",
    "\n",
    "# Start server\n",
    "process = subprocess.Popen([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'serve'\n",
    "], env=os.environ.copy())\n",
    "\n",
    "print(f\"Server started (PID: {process.pid})\")\n",
    "time.sleep(8)\n",
    "\n",
    "# Test server\n",
    "result = subprocess.run([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'list'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(f\"Server status: {result.returncode}\")\n",
    "print(f\"Models: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7ca79-51b7-4f6d-96a9-0763fca6784d",
   "metadata": {},
   "source": [
    "### Check the GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e2551-7d34-4139-b25c-fbbb23fc3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a3489-47ad-45d8-8176-24eaf7b83a31",
   "metadata": {},
   "source": [
    "### Download a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cb7ea-5a0e-429e-8c9a-9d2532824439",
   "metadata": {},
   "source": [
    "We will use the open source model OLMo2-0325-32B-Instruct. OLMo2 is a 32-billion parameter transformer-based language model developed by the Allen Institute for AI (AI2), released in March 2025 as part of the OLMo 2 family. This instruction-tuned variant represents a significant milestone as the first fully-open model to outperform GPT-3.5 Turbo and GPT-4o mini on a suite of popular academic benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b62e82-20f5-4510-a8fb-df019211b632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if result.returncode == 0:  # If server is working\n",
    "    print(\"Downloading model...\")\n",
    "    download_result = subprocess.run([\n",
    "        '/user/sarah.oberbichler/u18915/bin/ollama', 'pull', 'llama3.1:8b'\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    print(f\"Download result: {download_result.returncode}\")\n",
    "    if download_result.stderr:\n",
    "        print(f\"Download output: {download_result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed2383-96b8-48a8-9548-81885e43317a",
   "metadata": {},
   "source": [
    "### Get the model started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76c095-2956-4e6c-bf8c-5a214c01e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL for your local Ollama server\n",
    "API_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "#Set model and model parameters\n",
    "def ask_llama(prompt, model=\"llama3.1:8b\"):\n",
    "    import urllib.request, json\n",
    "    req = urllib.request.Request(\n",
    "        f\"{API_URL}/api/chat\",\n",
    "        json.dumps({\"model\": model, \"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
    "                    \"stream\": False, \"temperature\": 0.2}).encode(),\n",
    "        {\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    with urllib.request.urlopen(req) as r:\n",
    "        return json.loads(r.read())[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb776c97-a0ea-4564-99a9-f1f9a238a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "def warm_up_model():\n",
    "    \"\"\"Warm up the model with simple prompts.\"\"\"\n",
    "    print(\"Warming up model...\")\n",
    "    \n",
    "    for prompt in [\"Hi\", \"What is 2+2?\", \"Say hello\"]:\n",
    "        try:\n",
    "            ask_olmo_api(prompt)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\nWarmup complete!\")\n",
    "\n",
    "warm_up_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc00cd-f7ea-47f1-b260-79f33289f85e",
   "metadata": {},
   "source": [
    "### Chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aba6c-1cca-4979-8ed1-236693229354",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_llama('hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
