{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP511GgO/nopsxFJhLWpmt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/HF_Jobs_Setting_UP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running LLM Jobs via HuggingFace\n",
        "\n",
        "For explanations on Hugginface https://huggingface.co/docs/huggingface_hub/guides/jobs\n",
        "\n"
      ],
      "metadata": {
        "id": "lL56CWXjl3vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Requirements for Hugging Face Jobs\n",
        "\n",
        "\n",
        "\n",
        "*   Hugging Face Pro account - A paid subscription is required to access job creation features\n",
        "*   Write access token - Generate a token with write permissions from your account settings\n",
        "*   Valid payment method - Jobs consume compute credits based on usage\n",
        "\n",
        "\n",
        "##Authentication Setup\n",
        "\n",
        "\n",
        "\n",
        "*   Create your access token at huggingface.co/settings/tokens (you will be given an API as part of the workshop)\n",
        "*   Ensure the token has \"Write\" permissions enabled\n"
      ],
      "metadata": {
        "id": "tJA5bCH8pL9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare your HF Job Script:\n",
        "\n",
        "This script creates a remote computational job on HuggingFace's infrastructure that loads a language model and answers a question. It uses `run_job` to spin up a GPU-enabled Docker container (PyTorch with CUDA), installs necessary Python packages (transformers, accelerate, etc.), then runs a Python script that loads a chosen model, defines a question (\"What is machine learning and how does it work?\"), creates a simple Q&A function that formats the question as a chat prompt, generates an answer using the loaded model with specific parameters (temperature 0.7, max 500 tokens), and finally prints both the question and the model's response in a formatted output. Essentially, it's a way to run AI inference on powerful remote hardware without needing local GPU resources - you submit the job, it runs on HuggingFace's servers, and you get the AI-generated answer back."
      ],
      "metadata": {
        "id": "uAqtsk-ns8f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "\n",
        "We are using the deepseek-ai/DeepSeek-R1-Distill-Qwen-14B model in this environment.\n",
        "\n",
        "LLM Distillation is a technique where a smaller \"student\" model learns to mimic the behavior of a larger, more powerful \"teacher\" model. Think of it like an expert teacher passing their knowledge to a student who then becomes highly capable but more efficient.\n",
        "How it works:\n",
        "\n",
        "**Teacher Model:** A large, powerful model (in this case, DeepSeek-R1) that performs very well but is expensive to run\n",
        "Student Model: A smaller model (here, Qwen-14B) that learns from both the original training data AND the teacher's responses\n",
        "Knowledge Transfer: The student model learns to produce similar outputs to the teacher, capturing its \"reasoning style\" and capabilities\n",
        "\n",
        "**For DeepSeek-R1-Distill-Qwen-14B specifically:**\n",
        "\n",
        "Teacher: DeepSeek R1 (671 billion parameters)\n",
        "Student: Qwen-14B (14 billion parameters - much smaller and faster)\n",
        "Result: A 14B model that inherited DeepSeek R1's reasoning abilities but runs much more efficiently\n"
      ],
      "metadata": {
        "id": "BedwDszO6Jj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ***You also need to add the HF token in the script. Search for \"your_token\" and add your HF token there.***\n",
        "\n"
      ],
      "metadata": {
        "id": "4bvCK7We7c5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import run_job\n",
        "\n",
        "job = run_job(\n",
        "    image=\"pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel\",\n",
        "    command=[\n",
        "        \"bash\", \"-c\",\n",
        "        \"\"\"\n",
        "        apt-get update && apt-get install -y wget &&\n",
        "        pip install -q \"transformers>=4.51.0\" accelerate bitsandbytes huggingface_hub &&\n",
        "        python3 -c \"\n",
        "import os, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "\n",
        "# CONFIGURATION\n",
        "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B'\n",
        "\n",
        "# YOUR QUESTION HERE - CHANGE THIS!\n",
        "QUESTION = 'What is machine learning and how does it work?'\n",
        "\n",
        "# SETUP\n",
        "hf_token = os.environ.get('HUGGINGFACE_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "print('Loading model...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    token=hf_token\n",
        ")\n",
        "print('Model loaded successfully!')\n",
        "\n",
        "# SIMPLE Q&A FUNCTION\n",
        "def ask_question(question):\n",
        "    prompt = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful AI assistant. Answer questions clearly and concisely.<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "{question}<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "'''\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=500,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# ASK YOUR QUESTION AND GET ANSWER\n",
        "print('\\\\n' + '='*60)\n",
        "print(f'QUESTION: {QUESTION}')\n",
        "print('='*60)\n",
        "\n",
        "try:\n",
        "    answer = ask_question(QUESTION)\n",
        "    print(f'ANSWER: {answer}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {str(e)}')\n",
        "\n",
        "print('\\\\n' + '='*60)\n",
        "print('Job complete!')\n",
        "\"\n",
        "        \"\"\"\n",
        "    ],\n",
        "    flavor=\"a100-large\",\n",
        "    env={\"HUGGINGFACE_TOKEN\": \"your_token\"}\n",
        ")\n",
        "\n",
        "print(f\"Job submitted! ID: {job.id}\")\n",
        "print(f\"Monitor at: https://huggingface.co/jobs/oberbics/{job.id}\")"
      ],
      "metadata": {
        "id": "xoSgJNpkokBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import inspect_job, fetch_job_logs\n",
        "import time\n",
        "\n",
        "# Poll job status until it's done\n",
        "while True:\n",
        "    status = inspect_job(job_id=job.id).status.stage\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status in (\"COMPLETED\", \"ERROR\"):\n",
        "        break\n",
        "    time.sleep(10)\n",
        "\n",
        "# Fetch logs after completion\n",
        "print(\"\\n=== Job logs ===\")\n",
        "logs = list(fetch_job_logs(job_id=job.id))\n",
        "for line in logs:\n",
        "    print(line)\n"
      ],
      "metadata": {
        "id": "9EAgsLe5dh0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}