{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4896923-0a60-4520-a7bf-f8aaa364b207",
   "metadata": {},
   "source": [
    "# Analyse Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad098f-9a22-4561-ae7d-7e5cc13a7586",
   "metadata": {},
   "source": [
    "First, you need to connect to the Ollama server. Use your own acoount information: https://hpcproject.gwdg.de/projects/baaa32d3-6b49-4831-8b75-87ff44056ae0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b71f27c-fda2-400b-b842-46d3d8f685d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started (PID: 631407)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-09T23:22:09.696+02:00 level=INFO source=routes.go:1304 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY:*.hlrn.de,jupyter.hpc.gwdg.de,jupyter.usr.hpc.gwdg.de,localhost,127.0.0.1 OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/user/sarah.oberbichler/u18915/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy:http://www-cache.gwdg.de:3128 https_proxy:http://www-cache.gwdg.de:3128 no_proxy:]\"\n",
      "time=2025-09-09T23:22:09.713+02:00 level=INFO source=images.go:477 msg=\"total blobs: 10\"\n",
      "time=2025-09-09T23:22:09.715+02:00 level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-09-09T23:22:09.718+02:00 level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
      "time=2025-09-09T23:22:09.718+02:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-09-09T23:22:09.987+02:00 level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-7a477136-dee0-7105-40b5-0eefb2da62ae library=cuda variant=v12 compute=7.0 driver=12.4 name=\"Tesla V100S-PCIE-32GB\" total=\"31.7 GiB\" available=\"31.4 GiB\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/09 - 23:22:17 | 200 |      58.373µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/09 - 23:22:17 | 200 |    5.032106ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "Server status: 0\n",
      "Models: NAME                                              ID              SIZE      MODIFIED     \n",
      "llama3.1:8b                                       46e0c10c039e    4.9 GB    13 hours ago    \n",
      "MHKetbi/allenai_OLMo2-0325-32B-Instruct:q5_K_S    7c99a6b3ca4d    22 GB     31 hours ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Kill any existing processes\n",
    "!pkill -f ollama\n",
    "time.sleep(2)\n",
    "\n",
    "# Start server\n",
    "process = subprocess.Popen([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'serve'\n",
    "], env=os.environ.copy())\n",
    "\n",
    "print(f\"Server started (PID: {process.pid})\")\n",
    "time.sleep(8)\n",
    "\n",
    "# Test server\n",
    "result = subprocess.run([\n",
    "    '/user/sarah.oberbichler/u18915/bin/ollama', 'list'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(f\"Server status: {result.returncode}\")\n",
    "print(f\"Models: {result.stdout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa660f-1fde-429c-8ab4-2d2c08333249",
   "metadata": {},
   "source": [
    "### Load the dataset (SummerSchool_dataset.xlsx).\n",
    "You can find it here: https://github.com/soberbichler/Workshop_QualitativeDataResearch_LLM/blob/main/data/SummerSchool_dataset.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f97b74-2cb2-4eaf-9774-ef058cb10a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>newspaper_title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>access_provider</th>\n",
       "      <th>preservation_institution</th>\n",
       "      <th>language</th>\n",
       "      <th>license</th>\n",
       "      <th>full_text</th>\n",
       "      <th>context_window</th>\n",
       "      <th>extracted_articles</th>\n",
       "      <th>llm_training_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7Q7RJSFQ7L7L7GGJVYE3HVWSOY3RA5F5-FID-F_SBB_000...</td>\n",
       "      <td>Norddeutsche allgemeine Zeitung, Erste (Abend-...</td>\n",
       "      <td>1909-01-08 12:00:00</td>\n",
       "      <td>Deutsche Digitale Bibliothek</td>\n",
       "      <td>Staatsbibliothek zu Berlin - Preußischer Kultu...</td>\n",
       "      <td>german</td>\n",
       "      <td>public domain</td>\n",
       "      <td>V. «f fr erfeblgen. Im Einklang mit der Thronr...</td>\n",
       "      <td>er mittcilen, gestern die drei Bilder Angelo J...</td>\n",
       "      <td>&lt;article&gt;In der Urania hielt gestem Dr. P. Sch...</td>\n",
       "      <td>&lt;argument&gt;Die Erdbeben hätten nur einen unwese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGPJ5Q6N5GFBFLYDD7FGFNMOENVBACQK-uuid-4fafbc22...</td>\n",
       "      <td>The Daily record and the Dresden daily : the f...</td>\n",
       "      <td>1909-01-06 12:00:00</td>\n",
       "      <td>Deutsche Digitale Bibliothek</td>\n",
       "      <td>Sächsische Landesbibliothek - Staats- und Univ...</td>\n",
       "      <td>english</td>\n",
       "      <td>public domain</td>\n",
       "      <td>t • rt ... - v * ■ '%'«»» « % +** ■' r*^ f I I...</td>\n",
       "      <td>. Mein Leopold (Girardi) . ! ! ” 8 Urania Thea...</td>\n",
       "      <td>&lt;article&gt;King Victor Emanuel has issued a spec...</td>\n",
       "      <td>&lt;argument&gt;Several journalists who are not in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXQFFIG5JBIQLNPJUXC4TGNMMOBDFPBF-uuid-561ffd16...</td>\n",
       "      <td>The Daily record and the Dresden daily : the f...</td>\n",
       "      <td>1909-01-26 12:00:00</td>\n",
       "      <td>Deutsche Digitale Bibliothek</td>\n",
       "      <td>Sächsische Landesbibliothek - Staats- und Univ...</td>\n",
       "      <td>english</td>\n",
       "      <td>public domain</td>\n",
       "      <td>Office: StmveStr.5.L DresdenA. Telephone 1755....</td>\n",
       "      <td>HOP. Grenoble, January 25. As the Bishop, Mons...</td>\n",
       "      <td>&lt;article&gt;THE HAIL) RECORD FUND FOR THE RELIEF ...</td>\n",
       "      <td>&lt;argument&gt;Help is still badly needed for the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAPS57OUOVCP3BAYCZB4BPDEE7WOKV5B-FILE_0001_DDB...</td>\n",
       "      <td>The Daily record and the Dresden daily : the f...</td>\n",
       "      <td>1909-02-19 12:00:00</td>\n",
       "      <td>Deutsche Digitale Bibliothek</td>\n",
       "      <td>Sächsische Landesbibliothek - Staats- und Univ...</td>\n",
       "      <td>english</td>\n",
       "      <td>public domain</td>\n",
       "      <td>Office DresdenA. Telephone 1755. and THE DRESD...</td>\n",
       "      <td>N, 4, Reichs Strasse i10AQ ' succ. to Helena W...</td>\n",
       "      <td>&lt;article&gt;THE DAIUIECOHD FUND FOR THE RELIEF OF...</td>\n",
       "      <td>&lt;argument&gt;NA&lt;/argument&gt;\\n&lt;claim&gt;NA&lt;/claim&gt;\\n&lt;e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IGEESPJG6R6SEJPLIF7YIVBRDGZRIRUJ-uuid-b1178335...</td>\n",
       "      <td>The Daily record and the Dresden daily : the f...</td>\n",
       "      <td>1909-02-03 12:00:00</td>\n",
       "      <td>Deutsche Digitale Bibliothek</td>\n",
       "      <td>Sächsische Landesbibliothek - Staats- und Univ...</td>\n",
       "      <td>english</td>\n",
       "      <td>public domain</td>\n",
       "      <td>2 THE DAILY RECORD, WEDNESDAY, FEBRUARY 3, 190...</td>\n",
       "      <td>Mrs. John Rohn, and little daughter, are in D...</td>\n",
       "      <td>&lt;article&gt;Twenty-four injured persons who had b...</td>\n",
       "      <td>&lt;argument&gt;NA&lt;/argument&gt;\\n&lt;claim&gt;NA&lt;/claim&gt;\\n&lt;e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id  \\\n",
       "0  7Q7RJSFQ7L7L7GGJVYE3HVWSOY3RA5F5-FID-F_SBB_000...   \n",
       "1  EGPJ5Q6N5GFBFLYDD7FGFNMOENVBACQK-uuid-4fafbc22...   \n",
       "2  EXQFFIG5JBIQLNPJUXC4TGNMMOBDFPBF-uuid-561ffd16...   \n",
       "3  HAPS57OUOVCP3BAYCZB4BPDEE7WOKV5B-FILE_0001_DDB...   \n",
       "4  IGEESPJG6R6SEJPLIF7YIVBRDGZRIRUJ-uuid-b1178335...   \n",
       "\n",
       "                                     newspaper_title     publication_date  \\\n",
       "0  Norddeutsche allgemeine Zeitung, Erste (Abend-...  1909-01-08 12:00:00   \n",
       "1  The Daily record and the Dresden daily : the f...  1909-01-06 12:00:00   \n",
       "2  The Daily record and the Dresden daily : the f...  1909-01-26 12:00:00   \n",
       "3  The Daily record and the Dresden daily : the f...  1909-02-19 12:00:00   \n",
       "4  The Daily record and the Dresden daily : the f...  1909-02-03 12:00:00   \n",
       "\n",
       "                access_provider  \\\n",
       "0  Deutsche Digitale Bibliothek   \n",
       "1  Deutsche Digitale Bibliothek   \n",
       "2  Deutsche Digitale Bibliothek   \n",
       "3  Deutsche Digitale Bibliothek   \n",
       "4  Deutsche Digitale Bibliothek   \n",
       "\n",
       "                            preservation_institution language        license  \\\n",
       "0  Staatsbibliothek zu Berlin - Preußischer Kultu...   german  public domain   \n",
       "1  Sächsische Landesbibliothek - Staats- und Univ...  english  public domain   \n",
       "2  Sächsische Landesbibliothek - Staats- und Univ...  english  public domain   \n",
       "3  Sächsische Landesbibliothek - Staats- und Univ...  english  public domain   \n",
       "4  Sächsische Landesbibliothek - Staats- und Univ...  english  public domain   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  V. «f fr erfeblgen. Im Einklang mit der Thronr...   \n",
       "1  t • rt ... - v * ■ '%'«»» « % +** ■' r*^ f I I...   \n",
       "2  Office: StmveStr.5.L DresdenA. Telephone 1755....   \n",
       "3  Office DresdenA. Telephone 1755. and THE DRESD...   \n",
       "4  2 THE DAILY RECORD, WEDNESDAY, FEBRUARY 3, 190...   \n",
       "\n",
       "                                      context_window  \\\n",
       "0  er mittcilen, gestern die drei Bilder Angelo J...   \n",
       "1  . Mein Leopold (Girardi) . ! ! ” 8 Urania Thea...   \n",
       "2  HOP. Grenoble, January 25. As the Bishop, Mons...   \n",
       "3  N, 4, Reichs Strasse i10AQ ' succ. to Helena W...   \n",
       "4   Mrs. John Rohn, and little daughter, are in D...   \n",
       "\n",
       "                                  extracted_articles  \\\n",
       "0  <article>In der Urania hielt gestem Dr. P. Sch...   \n",
       "1  <article>King Victor Emanuel has issued a spec...   \n",
       "2  <article>THE HAIL) RECORD FUND FOR THE RELIEF ...   \n",
       "3  <article>THE DAIUIECOHD FUND FOR THE RELIEF OF...   \n",
       "4  <article>Twenty-four injured persons who had b...   \n",
       "\n",
       "                                 llm_training_answer  \n",
       "0  <argument>Die Erdbeben hätten nur einen unwese...  \n",
       "1  <argument>Several journalists who are not in t...  \n",
       "2  <argument>Help is still badly needed for the s...  \n",
       "3  <argument>NA</argument>\\n<claim>NA</claim>\\n<e...  \n",
       "4  <argument>NA</argument>\\n<claim>NA</claim>\\n<e...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('SummerSchool_dataset.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef7f265-2584-427c-ac27-f2ad79044d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up model...\n",
      "\n",
      "Warmup complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the model \n",
    "def warm_up_model():\n",
    "    \"\"\"Warm up the model with simple prompts.\"\"\"\n",
    "    print(\"Warming up model...\")\n",
    "    \n",
    "    for prompt in [\"Hi\", \"What is 2+2?\", \"Say hello\"]:\n",
    "        try:\n",
    "            ask_olmo_api(prompt)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\nWarmup complete!\")\n",
    "\n",
    "warm_up_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28a07a-8fd1-41a5-94e8-b0796947a13e",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "\n",
    "Add your prompt and you can also adjust the model parameter\n",
    "\n",
    "After running the cell, fill in the model documentation while waiting (and continue after saving the results)!\n",
    "\n",
    "Model documentation: https://seafile.rlp.net/seafhttp/f/a5b34ec61267408da431/?op=view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf142d9-33a4-4591-9102-8bbf792927d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-09T23:22:53.367+02:00 level=INFO source=sched.go:786 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 gpu=GPU-7a477136-dee0-7105-40b5-0eefb2da62ae parallel=1 available=33747566592 required=\"23.0 GiB\"\n",
      "time=2025-09-09T23:22:53.501+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"754.6 GiB\" free=\"729.9 GiB\" free_swap=\"0 B\"\n",
      "time=2025-09-09T23:22:53.502+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split=\"\" memory.available=\"[31.4 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"23.0 GiB\" memory.required.partial=\"23.0 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[23.0 GiB]\" memory.weights.total=\"20.4 GiB\" memory.weights.repeating=\"20.0 GiB\" memory.weights.nonrepeating=\"402.0 MiB\" memory.graph.full=\"853.3 MiB\" memory.graph.partial=\"853.3 MiB\"\n",
      "llama_model_loader: loaded meta data with 36 key-value pairs and 707 tensors from /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Open_Instruct_Dev\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Allenai\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   7:                  general.base_model.0.name str              = OLMo 2 0325 32B DPO\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Allenai\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/allenai/OLMo-2...\n",
      "llama_model_loader: - kv  10:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv  11:                     general.dataset.0.name str              = Tulu 3 Sft Olmo 2 Mixture\n",
      "llama_model_loader: - kv  12:             general.dataset.0.organization str              = Allenai\n",
      "llama_model_loader: - kv  13:                 general.dataset.0.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv  15:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  16:                          olmo2.block_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       olmo2.context_length u32              = 4096\n",
      "llama_model_loader: - kv  18:                     olmo2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  19:                  olmo2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  20:                 olmo2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  21:              olmo2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  22:                       olmo2.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  23:     olmo2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 100277\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 16\n",
      "llama_model_loader: - type  f32:  257 tensors\n",
      "llama_model_loader: - type q5_K:  449 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Small\n",
      "print_info: file size   = 20.71 GiB (5.52 BPW) \n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.6143 MB\n",
      "print_info: arch             = olmo2\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 32.23 B\n",
      "print_info: general.name     = Open_Instruct_Dev\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 100352\n",
      "print_info: n_merges         = 100000\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOT token        = 100257 '<|endoftext|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100277 '<|pad|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100265 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-09-09T23:22:53.720+02:00 level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/mnt/vast-nhr/home/sarah.oberbichler/u18915/bin/ollama runner --model /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 --ctx-size 4096 --batch-size 512 --n-gpu-layers 65 --threads 40 --parallel 1 --port 45705\"\n",
      "time=2025-09-09T23:22:53.720+02:00 level=INFO source=sched.go:481 msg=\"loaded runners\" count=1\n",
      "time=2025-09-09T23:22:53.720+02:00 level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-09-09T23:22:53.721+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
      "time=2025-09-09T23:22:53.736+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla V100S-PCIE-32GB, compute capability 7.0, VMM: yes\n",
      "load_backend: loaded CUDA backend from /mnt/vast-nhr/home/sarah.oberbichler/u18915/lib/ollama/libggml-cuda.so\n",
      "load_backend: loaded CPU backend from /mnt/vast-nhr/home/sarah.oberbichler/u18915/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-09-09T23:22:53.844+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-09-09T23:22:53.845+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:45705\"\n",
      "llama_model_load_from_file_impl: using device CUDA0 (Tesla V100S-PCIE-32GB) - 32184 MiB free\n",
      "time=2025-09-09T23:22:53.973+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: loaded meta data with 36 key-value pairs and 707 tensors from /user/sarah.oberbichler/u18915/.ollama/models/blobs/sha256-fd251f9b771983bcee3f8b52173daf0d5215521bb0f80478cb806100288db272 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Open_Instruct_Dev\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Allenai\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   7:                  general.base_model.0.name str              = OLMo 2 0325 32B DPO\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Allenai\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/allenai/OLMo-2...\n",
      "llama_model_loader: - kv  10:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv  11:                     general.dataset.0.name str              = Tulu 3 Sft Olmo 2 Mixture\n",
      "llama_model_loader: - kv  12:             general.dataset.0.organization str              = Allenai\n",
      "llama_model_loader: - kv  13:                 general.dataset.0.repo_url str              = https://huggingface.co/allenai/tulu-3...\n",
      "llama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv  15:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  16:                          olmo2.block_count u32              = 64\n",
      "llama_model_loader: - kv  17:                       olmo2.context_length u32              = 4096\n",
      "llama_model_loader: - kv  18:                     olmo2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  19:                  olmo2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  20:                 olmo2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  21:              olmo2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  22:                       olmo2.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  23:     olmo2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = dbrx\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 100257\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 100257\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 100277\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 16\n",
      "llama_model_loader: - type  f32:  257 tensors\n",
      "llama_model_loader: - type q5_K:  449 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Small\n",
      "print_info: file size   = 20.71 GiB (5.52 BPW) \n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.6143 MB\n",
      "print_info: arch             = olmo2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 64\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 27648\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 32B\n",
      "print_info: model params     = 32.23 B\n",
      "print_info: general.name     = Open_Instruct_Dev\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 100352\n",
      "print_info: n_merges         = 100000\n",
      "print_info: BOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOS token        = 100257 '<|endoftext|>'\n",
      "print_info: EOT token        = 100257 '<|endoftext|>'\n",
      "print_info: UNK token        = 100257 '<|endoftext|>'\n",
      "print_info: PAD token        = 100277 '<|pad|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
      "print_info: EOG token        = 100257 '<|endoftext|>'\n",
      "print_info: EOG token        = 100265 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: offloading 64 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 65/65 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size = 20865.97 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   336.88 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context:  CUDA_Host  output buffer size =     0.40 MiB\n",
      "llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context:      CUDA0 compute buffer size =   358.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    18.01 MiB\n",
      "llama_context: graph nodes  = 2438\n",
      "llama_context: graph splits = 2\n",
      "time=2025-09-09T23:23:00.740+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 7.02 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/09 - 23:23:11 | 200 | 18.453246841s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:20 | 200 |   9.29380947s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:26 | 200 |  5.915784658s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:31 | 200 |  4.983330333s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:38 | 200 |  6.386078573s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:43 | 200 |  5.723020291s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:48 | 200 |  4.559602464s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:23:53 | 200 |  5.349455885s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:24:00 | 200 |  7.137006921s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/09/09 - 23:24:09 | 200 |  8.485378921s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert at analyzing historical texts and you hate to summarize.\n",
    "Your task: find argumentative units in historical articles.\n",
    "Arguments in newspapers are often implicit but should contain a clear premise (with an inclusive claim).\n",
    "\n",
    "OUTPUT FORMAT - EXACTLY these 4 XML tags and NOTHING else:\n",
    "<argument>Original argument text OR \"NA\"</argument>\n",
    "<claim>Core claim (implication) in one sentence OR \"NA\"</claim>\n",
    "<explanation>Why this is an argument OR \"NA\"</explanation>\n",
    "<human_verification_needed>True OR False</human_verification_needed>\n",
    "\n",
    "EXAMPLE WITH ARGUMENT:\n",
    "<argument>Es sind furchtbare Bilder, die sich dabei entrollen. Unter den Trümmern des einen Hauses, so erzählt Luigi Barsini im Corriere della Sera, findet man die Leichen von Unglücklichen, die in anderen Häusern gewohnt haben und die in der Verwirrung des schrecklichen Augenblickes instinktiv bei Fremden Hilfe und Unterschlupf suchten. Niemand erkennt jetzt diese armen Eindringlinge, ihre Leichen werden nicht reklamiert, und man trägt sie hinunter an den Strand, wo sie in langer Reihe einer neben den anderen hingebettet werden, in denselben Tüchern und Decken, in denen sie ihren Tod gefunden.</argument>\n",
    "<claim>The earthquake's chaos led to unidentified victims dying in unfamiliar places.</claim>\n",
    "<explanation>Describes how people fled to other houses seeking help during the disaster, died there, and now cannot be identified or claimed by relatives. Shows cause (panic/confusion) and effect (anonymous deaths).</explanation>\n",
    "<human_verification_needed>False</human_verification_needed>\n",
    "\n",
    "EXAMPLE WITHOUT ARGUMENT:\n",
    "<argument>NA</argument>\n",
    "<claim>NA</claim>\n",
    "<explanation>NA</explanation>\n",
    "<human_verification_needed>False</human_verification_needed>\n",
    "\n",
    "RULES:\n",
    "- NO SUMMARY; ONLY ORIGINAL EXCERPT FROM THE TEXT; don't extract anything that is not in the text. Only extract word by word.\n",
    "- ONLY output these 4 XML tags.\n",
    "- Factual reportings such as \"Dem Vulkanausbruch folgten drei Sturzwellen in etwa 10 Meter Höhe\" are NO arguments.\n",
    "- Extract only original text without changes or use NA when you did not find an argument.\n",
    "- The claim is not a translation or summary of the argument. It should say what the (implicit) argument implies.\n",
    "- In cases of uncertainty or ambiguity, set <human_verification_needed>True</human_verification_needed>.\n",
    "- If no argument exists, use NA for all fields except <human_verification_needed>.\n",
    "- More than one argumentative unit possible per article; one unit has one clear claim and all four XML fields.\n",
    "\"\"\"\n",
    "\n",
    "def ask_olmo_api(text, temperature=0.1, max_tokens=1000):\n",
    "    \"\"\"Simple OLMo API call with system + user roles\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"MHKetbi/allenai_OLMo2-0325-32B-Instruct:q5_K_S\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract the arguments from this text:\\n{text}\"}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"num_predict\": max_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post('http://127.0.0.1:11434/api/chat', json=payload)\n",
    "        return response.json()['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Apply properly\n",
    "df['model_answer_olmo'] = df['extracted_articles'].apply(lambda x: ask_olmo_api(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc77660-2006-4053-b477-b40e6ed40f6c",
   "metadata": {},
   "source": [
    "### Export the Dataset and name it differently than \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a226d226-40fa-4ff4-b4ab-63b3ae540e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047510c-46dd-4037-bc12-9f670e80cfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
